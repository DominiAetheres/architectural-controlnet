{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59de8046",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from diffusers import StableDiffusionControlNetImg2ImgPipeline, ControlNetModel, UNet2DConditionModel\n",
    "from diffusers import DDPMScheduler, AutoencoderKL\n",
    "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb71602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "SKETCH_DIR = \"../../data/google_image/input\"\n",
    "IMAGE_DIR = \"../../data/google_image/target\"\n",
    "CONTROLNET_MODEL = \"lllyasviel/sd-controlnet-canny\"  # Pre-trained edge-based ControlNet\n",
    "SD_MODEL = \"runwayml/stable-diffusion-v1-5\"\n",
    "OUTPUT_DIR = \"final_model_2\"\n",
    "BATCH_SIZE = 2                   # Adjust based on GPU memory\n",
    "GRAD_ACCUM_STEPS = 2             # Accumulate gradients for larger effective batch size\n",
    "NUM_EPOCHS = 3\n",
    "LR = 1e-5\n",
    "RESOLUTION = 512                 # SD 1.5 uses 512x512\n",
    "MIXED_PRECISION = \"fp16\"         # Use \"no\" if GPU doesn't support fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103f5f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SketchDataset(Dataset):\n",
    "    def __init__(self, sketch_dir, image_dir, tokenizer, resolution=512):\n",
    "        self.sketch_paths = sorted(glob.glob(os.path.join(sketch_dir, \"*.png\")))\n",
    "        self.image_paths = sorted(glob.glob(os.path.join(image_dir, \"*.png\")))\n",
    "        self.tokenizer = tokenizer\n",
    "        self.resolution = resolution\n",
    "        self.prompt = \"photorealistic render, realistic, photograph, outside\"  # Fixed prompt\n",
    "        \n",
    "        # Verify dataset\n",
    "        if len(self.sketch_paths) != len(self.image_paths):\n",
    "            print(f\"Warning: Sketch count ({len(self.sketch_paths)}) doesn't match image count ({len(self.image_paths)})\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return min(len(self.sketch_paths), len(self.image_paths))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load sketch and ensure proper format\n",
    "        sketch = Image.open(self.sketch_paths[idx])\n",
    "        if sketch.mode != \"RGB\":\n",
    "            sketch = sketch.convert(\"RGB\")\n",
    "        sketch = sketch.resize((self.resolution, self.resolution))\n",
    "        \n",
    "        # Load real image\n",
    "        real_img = Image.open(self.image_paths[idx])\n",
    "        if real_img.mode != \"RGB\":\n",
    "            real_img = real_img.convert(\"RGB\")\n",
    "        real_img = real_img.resize((self.resolution, self.resolution))\n",
    "        \n",
    "        # Tokenize text\n",
    "        inputs = self.tokenizer(\n",
    "            self.prompt,\n",
    "            max_length=self.tokenizer.model_max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Convert to tensors and normalize\n",
    "        sketch_tensor = (torch.tensor(np.array(sketch)).float().permute(2, 0, 1) / 255.0 * 2.0 - 1.0)\n",
    "        real_tensor = (torch.tensor(np.array(real_img)).float().permute(2, 0, 1) / 127.5 - 1.0)\n",
    "        \n",
    "        return {\n",
    "            \"pixel_values\": real_tensor,\n",
    "            \"conditioning_pixel_values\": sketch_tensor,\n",
    "            \"input_ids\": inputs.input_ids.squeeze(0)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac04342d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "controlnet = ControlNetModel.from_pretrained(CONTROLNET_MODEL)\n",
    "tokenizer = CLIPTokenizer.from_pretrained(SD_MODEL, subfolder=\"tokenizer\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(SD_MODEL, subfolder=\"text_encoder\")\n",
    "vae = AutoencoderKL.from_pretrained(SD_MODEL, subfolder=\"vae\")\n",
    "unet = UNet2DConditionModel.from_pretrained(SD_MODEL, subfolder=\"unet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e611a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all models except ControlNet\n",
    "for param in controlnet.parameters():\n",
    "    param.requires_grad = True \n",
    "    \n",
    "for param in unet.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in text_encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in vae.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91cfc7c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNet2DConditionModel(\n",
       "  (conv_in): Conv2d(4, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (time_proj): Timesteps()\n",
       "  (time_embedding): TimestepEmbedding(\n",
       "    (linear_1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "    (act): SiLU()\n",
       "    (linear_2): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "  )\n",
       "  (down_blocks): ModuleList(\n",
       "    (0): CrossAttnDownBlock2D(\n",
       "      (attentions): ModuleList(\n",
       "        (0-1): 2 x Transformer2DModel(\n",
       "          (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
       "          (proj_in): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (transformer_blocks): ModuleList(\n",
       "            (0): BasicTransformerBlock(\n",
       "              (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn1): Attention(\n",
       "                (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "                (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
       "                (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn2): Attention(\n",
       "                (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "                (to_k): Linear(in_features=768, out_features=320, bias=False)\n",
       "                (to_v): Linear(in_features=768, out_features=320, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "              (ff): FeedForward(\n",
       "                (net): ModuleList(\n",
       "                  (0): GEGLU(\n",
       "                    (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
       "                  )\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                  (2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (proj_out): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (resnets): ModuleList(\n",
       "        (0-1): 2 x ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)\n",
       "          (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "        )\n",
       "      )\n",
       "      (downsamplers): ModuleList(\n",
       "        (0): Downsample2D(\n",
       "          (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): CrossAttnDownBlock2D(\n",
       "      (attentions): ModuleList(\n",
       "        (0-1): 2 x Transformer2DModel(\n",
       "          (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "          (proj_in): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (transformer_blocks): ModuleList(\n",
       "            (0): BasicTransformerBlock(\n",
       "              (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn1): Attention(\n",
       "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "                (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
       "                (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn2): Attention(\n",
       "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "                (to_k): Linear(in_features=768, out_features=640, bias=False)\n",
       "                (to_v): Linear(in_features=768, out_features=640, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "              (ff): FeedForward(\n",
       "                (net): ModuleList(\n",
       "                  (0): GEGLU(\n",
       "                    (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "                  )\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                  (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (proj_out): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (resnets): ModuleList(\n",
       "        (0): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(320, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
       "          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): Conv2d(320, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
       "          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "        )\n",
       "      )\n",
       "      (downsamplers): ModuleList(\n",
       "        (0): Downsample2D(\n",
       "          (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): CrossAttnDownBlock2D(\n",
       "      (attentions): ModuleList(\n",
       "        (0-1): 2 x Transformer2DModel(\n",
       "          (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "          (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (transformer_blocks): ModuleList(\n",
       "            (0): BasicTransformerBlock(\n",
       "              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn1): Attention(\n",
       "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn2): Attention(\n",
       "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_k): Linear(in_features=768, out_features=1280, bias=False)\n",
       "                (to_v): Linear(in_features=768, out_features=1280, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (ff): FeedForward(\n",
       "                (net): ModuleList(\n",
       "                  (0): GEGLU(\n",
       "                    (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "                  )\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                  (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (resnets): ModuleList(\n",
       "        (0): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(640, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): Conv2d(640, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "        )\n",
       "      )\n",
       "      (downsamplers): ModuleList(\n",
       "        (0): Downsample2D(\n",
       "          (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): DownBlock2D(\n",
       "      (resnets): ModuleList(\n",
       "        (0-1): 2 x ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (up_blocks): ModuleList(\n",
       "    (0): UpBlock2D(\n",
       "      (resnets): ModuleList(\n",
       "        (0-2): 3 x ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 2560, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (upsamplers): ModuleList(\n",
       "        (0): Upsample2D(\n",
       "          (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): CrossAttnUpBlock2D(\n",
       "      (attentions): ModuleList(\n",
       "        (0-2): 3 x Transformer2DModel(\n",
       "          (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "          (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (transformer_blocks): ModuleList(\n",
       "            (0): BasicTransformerBlock(\n",
       "              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn1): Attention(\n",
       "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn2): Attention(\n",
       "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_k): Linear(in_features=768, out_features=1280, bias=False)\n",
       "                (to_v): Linear(in_features=768, out_features=1280, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (ff): FeedForward(\n",
       "                (net): ModuleList(\n",
       "                  (0): GEGLU(\n",
       "                    (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "                  )\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                  (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (resnets): ModuleList(\n",
       "        (0-1): 2 x ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 2560, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 1920, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(1920, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): Conv2d(1920, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (upsamplers): ModuleList(\n",
       "        (0): Upsample2D(\n",
       "          (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): CrossAttnUpBlock2D(\n",
       "      (attentions): ModuleList(\n",
       "        (0-2): 3 x Transformer2DModel(\n",
       "          (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "          (proj_in): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (transformer_blocks): ModuleList(\n",
       "            (0): BasicTransformerBlock(\n",
       "              (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn1): Attention(\n",
       "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "                (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
       "                (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn2): Attention(\n",
       "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "                (to_k): Linear(in_features=768, out_features=640, bias=False)\n",
       "                (to_v): Linear(in_features=768, out_features=640, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "              (ff): FeedForward(\n",
       "                (net): ModuleList(\n",
       "                  (0): GEGLU(\n",
       "                    (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "                  )\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                  (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (proj_out): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (resnets): ModuleList(\n",
       "        (0): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 1920, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(1920, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
       "          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): Conv2d(1920, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(1280, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
       "          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): Conv2d(1280, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 960, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(960, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
       "          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): Conv2d(960, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (upsamplers): ModuleList(\n",
       "        (0): Upsample2D(\n",
       "          (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): CrossAttnUpBlock2D(\n",
       "      (attentions): ModuleList(\n",
       "        (0-2): 3 x Transformer2DModel(\n",
       "          (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
       "          (proj_in): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (transformer_blocks): ModuleList(\n",
       "            (0): BasicTransformerBlock(\n",
       "              (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn1): Attention(\n",
       "                (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "                (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
       "                (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn2): Attention(\n",
       "                (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "                (to_k): Linear(in_features=768, out_features=320, bias=False)\n",
       "                (to_v): Linear(in_features=768, out_features=320, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "              (ff): FeedForward(\n",
       "                (net): ModuleList(\n",
       "                  (0): GEGLU(\n",
       "                    (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
       "                  )\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                  (2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (proj_out): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (resnets): ModuleList(\n",
       "        (0): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 960, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(960, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)\n",
       "          (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1-2): 2 x ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)\n",
       "          (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): Conv2d(640, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (mid_block): UNetMidBlock2DCrossAttn(\n",
       "    (attentions): ModuleList(\n",
       "      (0): Transformer2DModel(\n",
       "        (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "        (proj_in): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (transformer_blocks): ModuleList(\n",
       "          (0): BasicTransformerBlock(\n",
       "            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn1): Attention(\n",
       "              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_out): ModuleList(\n",
       "                (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn2): Attention(\n",
       "              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_k): Linear(in_features=768, out_features=1280, bias=False)\n",
       "              (to_v): Linear(in_features=768, out_features=1280, bias=False)\n",
       "              (to_out): ModuleList(\n",
       "                (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (ff): FeedForward(\n",
       "              (net): ModuleList(\n",
       "                (0): GEGLU(\n",
       "                  (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (proj_out): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (resnets): ModuleList(\n",
       "      (0-1): 2 x ResnetBlock2D(\n",
       "        (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "        (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (nonlinearity): SiLU()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (conv_norm_out): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "  (conv_act): SiLU()\n",
       "  (conv_out): Conv2d(320, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Optimizer (only train ControlNet)\n",
    "optimizer = torch.optim.AdamW(controlnet.parameters(), lr=LR, weight_decay=0.05)\n",
    "dataset = SketchDataset(SKETCH_DIR, IMAGE_DIR, tokenizer, resolution=RESOLUTION)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "\n",
    "# Scheduler\n",
    "noise_scheduler = DDPMScheduler.from_pretrained(SD_MODEL, subfolder=\"scheduler\")\n",
    "\n",
    "# Training setup\n",
    "controlnet.to(device)\n",
    "text_encoder.to(device)\n",
    "vae.to(device)\n",
    "unet.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5dc514a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training with 36 examples\n",
      "Device: cuda, Batch size: 1, Accum steps: 4\n",
      "Using ControlNet: lllyasviel/sd-controlnet-canny\n"
     ]
    }
   ],
   "source": [
    "# Mixed precision\n",
    "scaler = torch.amp.GradScaler(\"cuda\", enabled=MIXED_PRECISION == \"fp16\")\n",
    "\n",
    "print(f\"Starting training with {len(dataset)} examples\")\n",
    "print(f\"Device: {device}, Batch size: {BATCH_SIZE}, Accum steps: {GRAD_ACCUM_STEPS}\")\n",
    "print(f\"Using ControlNet: {CONTROLNET_MODEL}\")\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec412987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Epoch: 0, Step: 0 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\python-venvs\\actual\\Lib\\site-packages\\diffusers\\configuration_utils.py:141: FutureWarning: Accessing config attribute `num_train_timesteps` directly via 'DDPMScheduler' object attribute is deprecated. Please access 'num_train_timesteps' over 'DDPMScheduler's config object instead, e.g. 'scheduler.config.num_train_timesteps'.\n",
      "  deprecate(\"direct config name access\", \"1.0.0\", deprecation_message, standard_warn=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.003892288077622652\n",
      "Loss: 0.02731848508119583\n",
      "Loss: 0.03729882836341858\n",
      "Loss: 0.020688112825155258\n",
      "Loss: 0.03698461502790451\n",
      "Loss: 0.009142505936324596\n",
      "Loss: 0.013230041600763798\n",
      "Loss: 0.0006327779847197235\n",
      "Loss: 0.004590954631567001\n",
      "Loss: 0.004443539772182703\n",
      "Loss: 0.1007956713438034\n",
      "Loss: 0.017717715352773666\n",
      "Loss: 0.0508512482047081\n",
      "Loss: 0.0012459794525057077\n",
      "Loss: 0.018883388489484787\n",
      "Loss: 0.010083112865686417\n",
      "Loss: 0.016805417835712433\n",
      "Loss: 0.047780923545360565\n",
      "Loss: 0.022824283689260483\n",
      "Loss: 0.01697903871536255\n",
      "Epoch 0, Step 5, Loss: 0.3698\n",
      "Loss: 0.0636143758893013\n",
      "Loss: 0.004050725139677525\n",
      "Loss: 0.002037922851741314\n",
      "Loss: 0.00931905023753643\n",
      "Loss: 0.005027182400226593\n",
      "Loss: 0.01643446832895279\n",
      "Loss: 0.015170787461102009\n",
      "Loss: 0.026057451963424683\n",
      "Loss: 0.017387758940458298\n",
      "Loss: 0.0304858535528183\n",
      "Loss: 0.006382010877132416\n",
      "Loss: 0.003982558846473694\n",
      "Loss: 0.0008740065386518836\n",
      "Loss: 0.04261345416307449\n",
      "Loss: 0.024621307849884033\n",
      "Loss: 0.08426712453365326\n",
      "===== Epoch: 1, Step: 9 =====\n",
      "Loss: 0.0018143217312172055\n",
      "Loss: 0.06544095277786255\n",
      "Loss: 0.016641534864902496\n",
      "Loss: 0.044063251465559006\n",
      "Epoch 1, Step 10, Loss: 0.1024\n",
      "Loss: 0.004632071126252413\n",
      "Loss: 0.0021430887281894684\n",
      "Loss: 0.03410983085632324\n",
      "Loss: 0.00948595441877842\n",
      "Loss: 0.0109107606112957\n",
      "Loss: 0.04949776083230972\n",
      "Loss: 0.010333908721804619\n",
      "Loss: 0.03134545683860779\n",
      "Loss: 0.02271527796983719\n",
      "Loss: 0.0009723143884912133\n",
      "Loss: 0.04190204292535782\n",
      "Loss: 0.018754223361611366\n",
      "Loss: 0.0066322339698672295\n",
      "Loss: 0.018226055428385735\n",
      "Loss: 0.011557716876268387\n",
      "Loss: 0.006546595133841038\n",
      "Loss: 0.047835901379585266\n",
      "Loss: 0.0008294032304547727\n",
      "Loss: 0.015458192676305771\n",
      "Loss: 0.00841759517788887\n",
      "Epoch 1, Step 15, Loss: 0.2818\n",
      "Loss: 0.012426316738128662\n",
      "Loss: 0.0036782799288630486\n",
      "Loss: 0.0029640670400112867\n",
      "Loss: 0.003949305973947048\n",
      "Loss: 0.0007349155494011939\n",
      "Loss: 0.006312709301710129\n",
      "Loss: 0.0028951894491910934\n",
      "Loss: 0.0025195209309458733\n",
      "Loss: 0.004127013497054577\n",
      "Loss: 0.06864866614341736\n",
      "Loss: 0.004126091953366995\n",
      "Loss: 0.045159049332141876\n",
      "===== Epoch: 2, Step: 18 =====\n",
      "Loss: 0.02479378506541252\n",
      "Loss: 0.0011954859364777803\n",
      "Loss: 0.0012721754610538483\n",
      "Loss: 0.018740929663181305\n",
      "Loss: 0.030866555869579315\n",
      "Loss: 0.018122034147381783\n",
      "Loss: 0.014070129953324795\n",
      "Loss: 0.0021673443261533976\n",
      "Epoch 2, Step 20, Loss: 0.0890\n",
      "Loss: 0.008354571647942066\n",
      "Loss: 0.014896253123879433\n",
      "Loss: 0.03044220060110092\n",
      "Loss: 0.004036924801766872\n",
      "Loss: 0.0005995180690661073\n",
      "Loss: 0.003968475386500359\n",
      "Loss: 0.01666760817170143\n",
      "Loss: 0.005229515954852104\n",
      "Loss: 0.036448366940021515\n",
      "Loss: 0.03305215388536453\n",
      "Loss: 0.0048307995311915874\n",
      "Loss: 0.007562568411231041\n",
      "Loss: 0.04824485629796982\n",
      "Loss: 0.09164167940616608\n",
      "Loss: 0.02262166142463684\n",
      "Loss: 0.03828510269522667\n",
      "Loss: 0.007812592200934887\n",
      "Loss: 0.004326386842876673\n",
      "Loss: 0.01331193558871746\n",
      "Loss: 0.14090290665626526\n",
      "Epoch 2, Step 25, Loss: 0.4266\n",
      "Loss: 0.004497976042330265\n",
      "Loss: 0.010419282130897045\n",
      "Loss: 0.033708665519952774\n",
      "Loss: 0.0028369261417537928\n",
      "Loss: 0.013543762266635895\n",
      "Loss: 0.06799903512001038\n",
      "Loss: 0.07653789967298508\n",
      "Loss: 0.004579671658575535\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "\n",
    "global_step = 0\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"===== Epoch: {epoch}, Step: {global_step} =====\")\n",
    "\n",
    "    controlnet.train()\n",
    "    total_loss = 0\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        # Move batch to device\n",
    "        pixel_values = batch[\"pixel_values\"].to(device)\n",
    "        conditioning_values = batch[\"conditioning_pixel_values\"].to(device)\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "\n",
    "        # Convert images to latents\n",
    "        with torch.no_grad():\n",
    "            latents = vae.encode(pixel_values).latent_dist.sample() * 0.18215\n",
    "            \n",
    "            # Encode text\n",
    "            encoder_hidden_states = text_encoder(input_ids)[0]\n",
    "\n",
    "        # Sample noise and timesteps\n",
    "        noise = torch.randn_like(latents)\n",
    "        timesteps = torch.randint(0, noise_scheduler.num_train_timesteps, (latents.shape[0],), device=device)\n",
    "        noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "        # ControlNet forward with mixed precision\n",
    "        with torch.amp.autocast(\"cuda\", enabled=MIXED_PRECISION == \"fp16\"):\n",
    "            # Forward through ControlNet\n",
    "            down_block_res_samples, mid_block_res_sample = controlnet(\n",
    "                noisy_latents,\n",
    "                timesteps,\n",
    "                encoder_hidden_states=encoder_hidden_states,\n",
    "                controlnet_cond=conditioning_values,\n",
    "                return_dict=False,\n",
    "            )\n",
    "            \n",
    "            # Predict noise residual\n",
    "            model_pred = unet(\n",
    "                noisy_latents,\n",
    "                timesteps,\n",
    "                encoder_hidden_states=encoder_hidden_states,\n",
    "                down_block_additional_residuals=down_block_res_samples,\n",
    "                mid_block_additional_residual=mid_block_res_sample,\n",
    "            ).sample\n",
    "        \n",
    "            # Compute loss\n",
    "            loss = torch.nn.functional.mse_loss(model_pred, noise, reduction=\"mean\")\n",
    "            loss = loss / GRAD_ACCUM_STEPS\n",
    "            print(\"Loss:\", loss.item())\n",
    "        \n",
    "        # Backpropagation with gradient accumulation\n",
    "        scaler.scale(loss).backward()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if (step + 1) % GRAD_ACCUM_STEPS == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            global_step += 1\n",
    "            \n",
    "            if global_step % 5 == 0:\n",
    "                avg_loss = total_loss * GRAD_ACCUM_STEPS / 5\n",
    "                print(f\"Epoch {epoch}, Step {global_step}, Loss: {avg_loss:.4f}\")\n",
    "                total_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af456eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete! Final model saved to final_model_2/final_model\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Save final model\n",
    "controlnet.save_pretrained(os.path.join(OUTPUT_DIR, \"final_model\"))\n",
    "print(f\"Training complete! Final model saved to {OUTPUT_DIR}/final_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc48a8fe",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc42d428",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0729901187b6426f836e46ce424aa0a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have disabled the safety checker for <class 'diffusers.pipelines.controlnet.pipeline_controlnet.StableDiffusionControlNetPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7ca75e255e844288e5f05e2cfb13732",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "# Load fine-tuned ControlNet\n",
    "controlnet = ControlNetModel.from_pretrained(\n",
    "    \"final_model/final_model\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\",\n",
    "    controlnet=controlnet,\n",
    "    torch_dtype=torch.float16,\n",
    "    safety_checker=None  # Disable if you get NSFW false positives\n",
    ").to(\"cuda\")\n",
    "\n",
    "# Memory optimizations\n",
    "pipe.enable_model_cpu_offload()\n",
    "\n",
    "# Process sketch\n",
    "sketch = Image.open(\"demo.png\").convert(\"RGB\").resize((512, 512))\n",
    "\n",
    "# Generate image\n",
    "image = pipe(\n",
    "    \"photorealistic render, realistic, photograph, outside angle, high quality, sharp details\",\n",
    "    image=sketch,\n",
    "    num_inference_steps=30,\n",
    "    guidance_scale=7.5,\n",
    "    generator=torch.Generator(device=\"cuda\").manual_seed(42)\n",
    ").images[0]\n",
    "\n",
    "image.save(\"output.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "actual",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
